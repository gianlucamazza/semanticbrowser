# Docker Development Environment - Quick Start

Complete development environment with Ollama (local LLM), Redis, and hot-reload.

## ğŸš€ Quick Start

```bash
# 1. Start all services (Ollama, Redis, Semantic Browser)
make docker-dev-up

# 2. Pull an Ollama model (required for LLM features)
make ollama-pull MODEL=llama3.2

# 3. Test the API
curl http://localhost:3000/

# 4. View logs
make docker-dev-logs
```

## ğŸ“¦ What's Included

- **Semantic Browser** (http://localhost:3000)
  - Hot-reload with cargo-watch
  - All features enabled
  - Debug logging

- **Ollama** (http://localhost:11434)
  - Local LLM inference
  - No API keys required
  - Multiple models support

- **Redis** (localhost:6379)
  - Token revocation
  - Caching layer

## ğŸ”§ Common Commands

### Service Management
```bash
make docker-dev-up          # Start environment
make docker-dev-down        # Stop environment
make docker-dev-restart     # Restart all services
make docker-dev-status      # Check service health
```

### Development
```bash
make docker-dev-logs        # View all logs
make docker-dev-logs-app    # View app logs only
make docker-dev-shell       # Open shell in container
make docker-dev-test        # Run quick API tests
```

### Ollama Models
```bash
make ollama-pull MODEL=llama3.2     # Download model
make ollama-list                    # List models
make ollama-run MODEL=llama3.2      # Interactive chat
make ollama-rm MODEL=llama3.2       # Remove model
```

### Maintenance
```bash
make docker-dev-build       # Rebuild image
make docker-dev-rebuild     # Force rebuild (no cache)
make docker-dev-clean       # Remove all data âš ï¸
make docker-dev-clean-cache # Clean cargo cache
```

## ğŸ“ Development Workflow

1. **Start environment**: `make docker-dev-up`
2. **Edit code** in `src/`
3. **Auto-rebuild** happens via cargo-watch
4. **Test changes**: `curl http://localhost:3000/`
5. **View logs**: `make docker-dev-logs-app`

## ğŸ¯ Recommended Models

| Model | Size | Use Case | Command |
|-------|------|----------|---------|
| llama3.2 | 8B | Best balance | `make ollama-pull MODEL=llama3.2` |
| llama3.2:1b | 1B | Fast testing | `make ollama-pull MODEL=llama3.2:1b` |
| mistral | 7B | Alternative | `make ollama-pull MODEL=mistral` |
| codellama | 7B | Code tasks | `make ollama-pull MODEL=codellama` |

## ğŸ› Troubleshooting

### Service won't start
```bash
make docker-dev-logs        # Check logs
make docker-dev-status      # Check health
```

### Port already in use
```bash
# Find process using port 3000
lsof -i :3000
# Kill it or change port in docker-compose.dev.yml
```

### Slow builds
```bash
# Cargo cache should persist
docker volume ls | grep cargo_cache

# If missing, rebuild will recreate
make docker-dev-rebuild
```

### Ollama model not found
```bash
# Pull the model first
make ollama-pull MODEL=llama3.2

# Verify
make ollama-list
```

## ğŸ“š Full Documentation

See [docs/developer-guide/docker-development.md](../docs/developer-guide/docker-development.md) for complete guide.

## ğŸ“ Examples

```bash
# Start environment
make docker-dev-up

# Pull model
make ollama-pull MODEL=llama3.2

# Test LLM endpoint
docker exec semantic-browser-dev cargo run --example streaming_example --all-features

# Test browser automation
docker exec semantic-browser-dev cargo run --example agent_with_browser --all-features

# Run tests
docker exec semantic-browser-dev cargo test --all-features
```

## ğŸ”— Service URLs

- **API**: http://localhost:3000
- **Ollama**: http://localhost:11434
- **Redis**: localhost:6379

## âš™ï¸ Configuration

Edit `docker/.env.dev` to customize:
- Logging levels
- Browser settings
- Model paths
- Resource limits

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Semantic Browser (Port 3000)   â”‚
â”‚  - Cargo Watch (Hot Reload)     â”‚
â”‚  - All Features Enabled         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚
         â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama    â”‚    â”‚    Redis    â”‚
â”‚  (11434)    â”‚    â”‚   (6379)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’¡ Tips

1. **First time**: Build takes 5-10 minutes (downloads dependencies)
2. **Incremental**: Rebuilds take 10-30 seconds
3. **Memory**: Allocate 16GB to Docker for Ollama
4. **Models**: Start with `llama3.2:1b` for fast testing
5. **Cache**: Don't delete cargo volumes unless necessary

## ğŸ†˜ Need Help?

- Check status: `make docker-dev-status`
- View logs: `make docker-dev-logs`
- Full guide: [docker-development.md](../docs/developer-guide/docker-development.md)
- Open issue: [GitHub Issues](https://github.com/gianlucamazza/semanticbrowser/issues)

---

**Happy coding! ğŸš€**
