# Docker Compose Development Configuration for Semantic Browser
# Uses Compose Specification (no version required for Docker Compose v2+)
#
# This configuration provides a complete development environment with:
# - Ollama for local LLM inference (no API keys needed)
# - Redis for caching and token revocation
# - Hot-reload for rapid development
# - Verbose logging for debugging
#
# Usage:
#   docker-compose -f docker/docker-compose.dev.yml up
#   or use the helper script:
#   ./docker/scripts/docker-dev.sh up

services:
  # ==============================================================================
  # Ollama - Using Host Installation
  # ==============================================================================
  # NOTE: Ollama service is commented out to use the host's Ollama instance
  # The semantic_browser container will connect to host.docker.internal:11434
  # Make sure Ollama is running on your host: ollama serve
  #
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: semantic-browser-ollama-dev
  #   ...
  # (service definition removed to use host Ollama)

  # ==============================================================================
  # Redis - Caching and Token Revocation
  # ==============================================================================
  redis:
    image: redis:7-alpine
    container_name: semantic-browser-redis-dev
    restart: unless-stopped

    ports:
      - "6379:6379"

    volumes:
      - redis_data:/data

    networks:
      - semantic_dev_net

    command: >
      redis-server
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec

    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s

    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M

  # ==============================================================================
  # Semantic Browser - Development Build with Hot Reload
  # ==============================================================================
  semantic_browser:
    build:
      context: ..
      dockerfile: docker/Dockerfile.dev
      args:
        # Enable development features (excluding telemetry to avoid OTLP connection issues)
        CARGO_FEATURES: "browser-automation,redis-integration,onnx-integration"
    image: semantic-browser:dev
    container_name: semantic-browser-dev
    restart: unless-stopped

    ports:
      - "3000:3000"

    environment:
      # Logging configuration - verbose for development
      RUST_LOG: ${RUST_LOG:-debug,semantic_browser=trace}
      RUST_BACKTRACE: ${RUST_BACKTRACE:-1}

      # Development mode
      DEVELOPMENT_MODE: "true"

      # LLM Configuration - Ollama (using host)
      # host.docker.internal resolves to the host machine's IP
      OLLAMA_API_BASE: http://host.docker.internal:11434

      # Redis Configuration
      REDIS_URL: redis://redis:6379

      # JWT Configuration (development secret - NOT FOR PRODUCTION)
      JWT_SECRET: ${JWT_SECRET:-dev-secret-key-min-32-chars-1234567890}

      # Knowledge Graph persistence
      KG_PERSIST_PATH: /data/kg

      # ML Models (optional)
      NER_MODEL_PATH: ${NER_MODEL_PATH:-}
      KG_INFERENCE_MODEL_PATH: ${KG_INFERENCE_MODEL_PATH:-}

      # Browser automation configuration
      BROWSER_HEADLESS: ${BROWSER_HEADLESS:-true}
      BROWSER_TIMEOUT_SECS: ${BROWSER_TIMEOUT_SECS:-60}
      BROWSER_POOL_SIZE: ${BROWSER_POOL_SIZE:-2}

      # PyO3 configuration
      PYO3_USE_ABI3_FORWARD_COMPATIBILITY: "1"

    volumes:
      # Mount source code for hot reload (read-only to prevent container from modifying host)
      - ../src:/app/src:ro
      - ../Cargo.toml:/app/Cargo.toml:ro
      - ../Cargo.lock:/app/Cargo.lock:ro

      # Mount examples for testing
      - ../examples:/app/examples:ro

      # Persistent data for Knowledge Graph
      - kg_data_dev:/data/kg

      # Optional: mount ML models
      - ../models:/models:ro

      # Optional: mount custom configuration
      - ../config:/config:ro

      # Cargo cache for faster rebuilds
      - cargo_cache:/usr/local/cargo/registry
      - cargo_git_cache:/usr/local/cargo/git

      # Build output cache (significantly speeds up rebuilds)
      - target_cache:/app/target

    networks:
      - semantic_dev_net

    depends_on:
      redis:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 15s

    # Resource limits (generous for development)
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G
        reservations:
          cpus: "1"
          memory: 1G

# ==============================================================================
# Volumes
# ==============================================================================
volumes:
  # Ollama models - NOT USED (using host Ollama)
  # ollama_models:
  #   driver: local

  # Redis data - persistent across restarts
  redis_data:
    driver: local

  # Knowledge Graph data - development instance
  kg_data_dev:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${KG_DATA_PATH:-./data/kg-dev}

  # Cargo caches - dramatically speeds up rebuilds
  cargo_cache:
    driver: local

  cargo_git_cache:
    driver: local

  target_cache:
    driver: local

# ==============================================================================
# Networks
# ==============================================================================
networks:
  semantic_dev_net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/16
